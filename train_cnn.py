import argparse
import json
import os
import random

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Train and evaluate a CNN model.")
    parser.add_argument(
        "--train-csv",
        default="data_clean/train.csv",
        help="Path to train.csv generated by preprocess.py",
    )
    parser.add_argument(
        "--test-csv",
        default="data_clean/test.csv",
        help="Path to test.csv generated by preprocess.py",
    )
    parser.add_argument(
        "--images-dir",
        default="data_clean/images",
        help="Directory containing resized images",
    )
    parser.add_argument(
        "--output-dir",
        default="artifacts",
        help="Directory to save model and metrics",
    )
    parser.add_argument("--img-size", type=int, default=96)
    parser.add_argument("--batch-size", type=int, default=16)
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument(
        "--max-samples",
        type=int,
        default=300,
        help="Limit number of training samples for quick runs",
    )
    parser.add_argument("--val-split", type=float, default=0.15)
    parser.add_argument("--seed", type=int, default=42)
    return parser.parse_args()


def seed_everything(seed: int) -> None:
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)


def resolve_path(path: str, images_dir: str) -> str:
    if os.path.isabs(path):
        return path
    return os.path.join(images_dir, os.path.basename(path))


def build_dataset(paths, labels, images_dir, img_size, batch_size, shuffle) -> tf.data.Dataset:
    path_ds = tf.data.Dataset.from_tensor_slices((paths, labels))
    if shuffle:
        path_ds = path_ds.shuffle(buffer_size=len(paths), seed=123)

    def _load_image(path, label):
        full_path = tf.numpy_function(
            lambda p: resolve_path(p.decode("utf-8"), images_dir),
            [path],
            tf.string,
        )
        image = tf.io.read_file(full_path)
        image = tf.image.decode_image(image, channels=3, expand_animations=False)
        image.set_shape([None, None, 3])
        image = tf.image.resize(image, (img_size, img_size))
        image = tf.cast(image, tf.float32) / 255.0
        return image, label

    ds = path_ds.map(_load_image, num_parallel_calls=tf.data.AUTOTUNE)
    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return ds


def build_model(num_classes: int, img_size: int) -> tf.keras.Model:
    inputs = tf.keras.Input(shape=(img_size, img_size, 3))
    x = tf.keras.layers.Conv2D(32, 3, activation="relu", padding="same")(inputs)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(64, 3, activation="relu", padding="same")(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(128, 3, activation="relu", padding="same")(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(256, 3, activation="relu", padding="same")(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dropout(0.3)(x)
    x = tf.keras.layers.Dense(128, activation="relu")(x)
    outputs = tf.keras.layers.Dense(num_classes, activation="softmax")(x)
    return tf.keras.Model(inputs, outputs)


def main() -> None:
    args = parse_args()

    if os.path.isfile("data_clean") and args.train_csv.startswith("data_clean/"):
        args.train_csv = args.train_csv.replace("data_clean/", "data_clean_dir/")
        args.test_csv = args.test_csv.replace("data_clean/", "data_clean_dir/")
        args.images_dir = args.images_dir.replace("data_clean/", "data_clean_dir/")

    os.makedirs(args.output_dir, exist_ok=True)
    seed_everything(args.seed)

    train_df = pd.read_csv(args.train_csv)
    test_df = pd.read_csv(args.test_csv)

    if args.max_samples > 0 and len(train_df) > args.max_samples:
        per_class = max(1, args.max_samples // train_df["label"].nunique())
        train_df = (
            train_df.groupby("label", group_keys=False)
            .apply(lambda group: group.sample(n=min(len(group), per_class), random_state=args.seed))
            .sample(frac=1, random_state=args.seed)
            .reset_index(drop=True)
        )

    labels = sorted(train_df["label"].unique())
    label_map = {label: idx for idx, label in enumerate(labels)}
    train_df["label_idx"] = train_df["label"].map(label_map)
    test_df["label_idx"] = test_df["label"].map(label_map)

    train_df, val_df = train_test_split(
        train_df,
        test_size=args.val_split,
        random_state=args.seed,
        stratify=train_df["label_idx"],
    )

    train_ds = build_dataset(
        train_df["image_path"].values,
        train_df["label_idx"].values,
        args.images_dir,
        args.img_size,
        args.batch_size,
        shuffle=True,
    )
    val_ds = build_dataset(
        val_df["image_path"].values,
        val_df["label_idx"].values,
        args.images_dir,
        args.img_size,
        args.batch_size,
        shuffle=False,
    )
    test_ds = build_dataset(
        test_df["image_path"].values,
        test_df["label_idx"].values,
        args.images_dir,
        args.img_size,
        args.batch_size,
        shuffle=False,
    )

    model = build_model(len(label_map), args.img_size)
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )

    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            filepath=os.path.join(args.output_dir, "model.keras"),
            save_best_only=True,
            monitor="val_accuracy",
            mode="max",
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor="val_loss", patience=5, restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor="val_loss", factor=0.5, patience=2
        ),
    ]

    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=args.epochs,
        callbacks=callbacks,
    )

    test_loss, test_acc = model.evaluate(test_ds, verbose=0)

    y_true = test_df["label_idx"].values
    y_pred = np.argmax(model.predict(test_ds), axis=1)

    report = classification_report(
        y_true, y_pred, target_names=labels, output_dict=True
    )
    conf_matrix = confusion_matrix(y_true, y_pred)

    with open(os.path.join(args.output_dir, "label_map.json"), "w") as f:
        json.dump(label_map, f, indent=2)

    metrics = {
        "test_loss": float(test_loss),
        "test_accuracy": float(test_acc),
    }
    with open(os.path.join(args.output_dir, "metrics.json"), "w") as f:
        json.dump(metrics, f, indent=2)

    report_path = os.path.join(args.output_dir, "classification_report.json")
    with open(report_path, "w") as f:
        json.dump(report, f, indent=2)

    conf_path = os.path.join(args.output_dir, "confusion_matrix.csv")
    pd.DataFrame(conf_matrix, index=labels, columns=labels).to_csv(conf_path)

    plot_training_curves(history, args.output_dir)

    print("Saved model to:", os.path.join(args.output_dir, "model.keras"))
    print("Saved metrics to:", os.path.join(args.output_dir, "metrics.json"))


def plot_training_curves(history: tf.keras.callbacks.History, output_dir: str) -> None:
    import matplotlib.pyplot as plt

    plt.figure(figsize=(8, 4))
    plt.plot(history.history["loss"], label="Train Loss")
    plt.plot(history.history["val_loss"], label="Val Loss")
    plt.legend()
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "loss_curve.png"), dpi=150)
    plt.close()

    plt.figure(figsize=(8, 4))
    plt.plot(history.history["accuracy"], label="Train Acc")
    plt.plot(history.history["val_accuracy"], label="Val Acc")
    plt.legend()
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "accuracy_curve.png"), dpi=150)
    plt.close()


if __name__ == "__main__":
    main()
